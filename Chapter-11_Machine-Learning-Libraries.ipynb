{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chpt12-Machine.Learning.Libraries.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPw9W1CofjV3luCQ1MQo3me",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kbehrman/foundational-python-for-data-science/blob/main/Chapter-12%3AMachine-Learning-Libraries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMO7TTN-Cxhm"
      },
      "source": [
        "## Notes\n",
        "* Tensorflow\n",
        "    - Developed for internal use at Google\n",
        "    - Based on Nueral networks\n",
        "* Keras\n",
        "    - opened source\n",
        "    - can be run on top of Tensorflow\n",
        "    - specialized for neural networks?\n",
        "    - designed by google engineer?\n",
        "    - [Getting Started with Keras](https://youtu.be/J6Ok8p463C4)\n",
        "* Pytorch\n",
        "    - ease of use\n",
        "    - Developed at Facebook\n",
        "    - computer vision, natural language processing\n",
        "    - based on torch C library\n",
        "* Scikit-learn\n",
        "    - Built on numpy and scipy\n",
        "    - popular for classical algorithms\n",
        "    - [Getting started](https://youtu.be/rvVkVsG49uU)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gaj5ZbwGcGpg"
      },
      "source": [
        "## Introduction\n",
        "Machine learning consists of letting a computer find a way to solve a problem using data. This contrasts with traditional programming where the programmer defines the means of solution in code. In this chapter, we will take a overview of some of the more popular libraries used for machine learning. These libraries implement the algorithms used to create and train machine learning model. These models have variuos uses, dependeding on the type of problem. Some models are useful for predicting future values, others for classifying data into groups or categories. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeSQnKuwd8mK"
      },
      "source": [
        "## Popular Libraries\n",
        "\n",
        "Four of the most popular libraries are TensorFlow, Keras, Pytorch, and Scikit-learn. TensorFlow was developed by Google for internal use. It is a powerful library used to solve problems using deep learning. This involves defining layers that transform the data and which are tuned as the solution is fit to data. Keras is an opened source library designed to work with TensorFlow, and it is now included in the TensorFlow library. \n",
        "\n",
        "PyTorch is Facebook's contribution to production worthy machone learning libraries. It is based on the Torch library, which makes use of GPUs in solving parralell problems.\n",
        "\n",
        "Scikit-learn is a popular library for starting machine learning. It is built on top of NumPy and SciPy. It has classes for most of the traditional algorithms. We will take a closer look at Scikit-learn, but first lets talk about a general approach to solving a problem using machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waxOj9jYhpA9"
      },
      "source": [
        "## High Level Process\n",
        "\n",
        "Machine learning algorithmns can be divided into two types, unsupervised and supervised learning. Unsupervised learning involes discovering insights about data without pre-existing results to test against. In supervised learning, you use known data to train and test a model. Generally the steps to trainig a supervised model are:\n",
        "\n",
        "1. Transform data\n",
        "2. Separate out test data\n",
        "3. Train the model\n",
        "4. Test accuracy\n",
        "\n",
        "Scikit-learn has tools to simplify each of these steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2sRiVjRiIFo"
      },
      "source": [
        "# Transformations\n",
        "\n",
        "For some algorithms it is advantagous to transform the data before training a model. For example you might want to take a continuous variable, such as age, and turn it into discreat catagories, such as age ranges. Scikit-learn includes many types of transformers, including ones for cleaning, feature extraction, reduction and expansion. These are represented as classes which generally use a .fit() method determine the transformation and a .transform() method to modify data. In figure ... we use a MinMaxScaler. This transformer scales values to fit in a defined range, between 0 and 1 by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzJpXtnrl9JO",
        "outputId": "1a7eeae1-fd4b-48e5-cf95-e342b56661bd"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.array([[100, 34, 4],\n",
        "          [90,  2,  0],\n",
        "          [78,  -12, 16],\n",
        "          [23,   45,   4]])\n",
        "\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[100,  34,   4],\n",
              "       [ 90,   2,   0],\n",
              "       [ 78, -12,  16],\n",
              "       [ 23,  45,   4]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKzFAL3ZkYTN",
        "outputId": "32cb7c93-64b8-4cdc-80bd-59b44b4105c9"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "minMax = MinMaxScaler()\n",
        "scaler = minMax.fit(data)\n",
        "\n",
        "scaler.transform(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.80701754, 0.25      ],\n",
              "       [0.87012987, 0.24561404, 0.        ],\n",
              "       [0.71428571, 0.        , 1.        ],\n",
              "       [0.        , 1.        , 0.25      ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI1EMzrDxCqa"
      },
      "source": [
        "There may be times you wish to seperate your data before fitting the tranformer. In this way the tranformer settings will not be effected by the test data. Since the fitting and transforming are seperate methods, it is easy to fit to the train data and use that to transform the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSLZ6RELyJGk"
      },
      "source": [
        "## Splitting test and training data\n",
        "\n",
        "One important pitfall to avoid when training a model is over-fitting. This is when a model perfectly predicts the data used to train it, but has little predictive power with new data. In the simplest sense, we avoid overfitting by not testing the model with the data that it was trained upon. Scikit-learn offers helper methods to make splitting data easy. In figure 12.2 we use the Scikit-learn function train_test_split() to split the iris data set provided with the library, into train and test data sets. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETi1ILPgC_Ff"
      },
      "source": [
        "Loading a sample dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVcKxfeLCp-8"
      },
      "source": [
        "from sklearn import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2uedwGxK5ii"
      },
      "source": [
        "data, target = datasets.load_iris(return_X_y=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veA82zfHLyFT",
        "outputId": "0e6e1bbe-f24d-4848-8cff-a08a7f5b93c6"
      },
      "source": [
        "print(type(data))\n",
        "print(data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(150, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LcZhSj_LzT0",
        "outputId": "5a05649f-74d9-4503-bcc3-c86b60728e3b"
      },
      "source": [
        "print(type(target))\n",
        "print(target.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(150,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5olkLFTtDMc9"
      },
      "source": [
        "Splitting data into training and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1j3Bfo3L6gT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "616efdc7-780c-4c18-8735-ebaf08dfeecc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, test_data, train_target, test_target = train_test_split(data, target)\n",
        "print(train_data.shape)\n",
        "print(train_target.shape)\n",
        "print(test_data.shape)\n",
        "print(test_target.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(112, 4)\n",
            "(112,)\n",
            "(38, 4)\n",
            "(38,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B40MlhyJdf0F"
      },
      "source": [
        "## Training a model\n",
        "\n",
        "Scikit-learn offers many classes representing various machine learning algorithms. These are refered to as estimators. Many estimators can be tuned using parameters during instantiation. Each estimator has a .fit() method which trains the model. Most of the .fit() methods take two arguments. The first is some sort of training data, refered to as samples.The second is the results, or targets, for those samples. Both arguments should be an array like objects, such as a NumPy arrays. Once the training is done, the model can predict results using it's .predict() method. The accuracy of this prediction can be checked using functions from the methods module. Figure 12. shows a simple example using the KNeighborsClassifier estimator. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENT6ay4o91ha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aebbcc5-ab84-4651-eb27-06396a5c4a13"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "knn.fit(train_data, train_target)\n",
        "test_prediction = knn.predict(test_data)\n",
        "metrics.accuracy_score(test_target, test_prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnih6Fx1Jf4Y"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "We have only touched the surface of Scikit-learns capability. Other important features are tools for cross-validation, where a data set is split multiple times to avoid over-fitting on test data, and Pipelines which wrap up transformers, estimators and cross-validation together. If you want to learn more about Scikit-learn, there are great tutorials on the official webiste, [scikit-learn tutorials](https://scikit-learn.org/stable/tutorial/index.html#tutorial-menu)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P6xFNBL_5Sw"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Many of the algorthims used to create Machine Learning models are represented in the major Python Machine Learning libraries. TensorFlow is a deep learning library created by Google. PyTorch is a library built on Torch by Facebook. Scikit-learn is popular library for getting started with Machine Learning. It has modules and functions to perform the steps of creating and analysing a model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYLqzHmABG6X"
      },
      "source": [
        "## Questions\n",
        "\n",
        "1. In which step of training a supervised estimator would a Scikit-learn Transformer be useful?\n",
        "\n",
        "2. Why is it important to separate training and test data?\n",
        "\n",
        "3. Once you have transformed your data and trained your model, what should you do next?"
      ]
    }
  ]
}
